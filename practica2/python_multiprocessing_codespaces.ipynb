{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVl91tpNYssb"
   },
   "source": [
    "# Parallel processing in Python\n",
    "\n",
    "The number of libraries and packages for Parallel Processing in Python is huge. Check https://wiki.python.org/moin/ParallelProcessing to get the general picture.\n",
    "\n",
    "In this session we present `multiprocessing`, one of the most important frameworks to implement parallel applications in Python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uxp63V94ypR-"
   },
   "source": [
    "# Python multiprocessing module\n",
    "\n",
    "The `multiprocessing` module (https://docs.python.org/3/library/multiprocessing.html) is a powerful and versatile library included in Python's standard distribution, designed to facilitate concurrent execution by spawning multiple processes. It provides a high-level interface for creating and managing processes, as well as a wide array of low-level tools to build complex systems for inter-process communication and synchronization.\n",
    "\n",
    "Unlike threads, which operate within the same memory space and are subject to the Global Interpreter Lock (GIL), `multiprocessing` spawns separate processes with individual memory spaces. This allows true parallelism on multi-core processors. The ideal case would be running one process per physical processor. In this case, each process would really be executed in parallel.\n",
    "\n",
    "The module includes tools for:\n",
    "\n",
    "- Creating and managing processes.\n",
    "- Sharing data between processes using shared memory or managed objects.\n",
    "- Establishing communication channels like Pipes and Queues.\n",
    "- Synchronizing operations with primitives such as Locks, Events, Semaphores, and Conditions.\n",
    "\n",
    "The library provides both low-level primitives for advanced users and high-level abstractions for easier use. The `Process` class is the main low level interface for creating processes. Higher-level constructs like `Pools` allow for an easy management of groups of worker processes, without the caveats of managing low-level interprocess communication and syncronization tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBzWDv5GtxHI"
   },
   "source": [
    "---\n",
    "Part of the code in this notebook is modified from (and further information in):\n",
    "\n",
    "- Python 201, Michael Driscoll\n",
    "\n",
    "- https://www.machinelearningplus.com/python/parallel-processing-python/\n",
    "\n",
    "- https://github.com/mmckerns/tuthpc\n",
    "\n",
    "- https://github.com/csc-training/hpc-python. Repository of course: PYTHON IN HIGH PERFORMANCE COMPUTING PARTNERSHIP FOR ADVANCED COMPUTING IN EUROPE (PRACE)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2bdUt3vyBuA"
   },
   "source": [
    "### Guess the number of processors in your computer\n",
    "\n",
    "The number of processes should be related to the number of physical processors in the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 703,
     "status": "ok",
     "timestamp": 1735819518078,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "7GwWxFkM6_Hs",
    "outputId": "b93211b7-7549-4dc0-f92d-081517874860"
   },
   "outputs": [],
   "source": [
    "# check the number of cores\n",
    "import multiprocessing as mp\n",
    "\n",
    "# multiprocessing cpu_count just provides the logical cores\n",
    "print(\"Number of logical cores: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1735819542550,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "Q0h8yc6Ay3l_",
    "outputId": "ba5d2b10-a1fe-4432-beae-b0a428d9b4c6"
   },
   "outputs": [],
   "source": [
    "# use psutil for more detailed information\n",
    "import psutil\n",
    "\n",
    "# Number of logical cores\n",
    "logical_cores_psutil = psutil.cpu_count(logical=True)\n",
    "\n",
    "# Number of physical cores\n",
    "physical_cores_psutil = psutil.cpu_count(logical=False)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Logical Cores (psutil): {logical_cores_psutil}\")\n",
    "print(f\"Physical Cores (psutil): {physical_cores_psutil}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzfYtz2tASt0"
   },
   "source": [
    "## The Process Class (low level process parallelism)\n",
    "\n",
    "The Process class allows to create a series of processes that call a given function(s).\n",
    "\n",
    "You have to create and `start()` the process. Then, just call the `join()` method on each process, which tells Python to wait for the process to terminate. If you need to stop a process, you can call its `terminate()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1735819764418,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "TxfUJRkpzX4r",
    "outputId": "98362b7b-99c4-4235-e0d6-0cc2b94f1113"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "import os\n",
    "\n",
    "def doubler(number):\n",
    "  \"\"\"\n",
    "  A doubling function that can be used by a process\n",
    "  \"\"\"\n",
    "  result = number * 2\n",
    "  proc = os.getpid()\n",
    "  print(f'{number} doubled to {result} by process id: {proc}')\n",
    "\n",
    "numbers = [5, 10, 15, 20, 25]\n",
    "\n",
    "procs_list = []\n",
    "# each process could receive a different target function and/or data args\n",
    "for index, number in enumerate(numbers):\n",
    "  p = Process(target=doubler, args=[number]) # args requieres an iterable\n",
    "  procs_list.append(p)\n",
    "  p.start()\n",
    "\n",
    "# wait for the workers to end\n",
    "for p in procs_list:\n",
    "  p.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1nkg_KaNkzu"
   },
   "source": [
    "However, with this approach, getting the output values of each process would requiere to instantiate a `multiprocessing.Queue` or `multiprocessing.Manager`. So this is a complex aproach, which is mainly used for spawning unrelated processes working on their own (typically, not the case in scientific computing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "def tarea():\n",
    "    return 42  # This value can NOT be retrieved directly\n",
    "\n",
    "p = Process(target=tarea)\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates how the child process can send a value to the parent process using a Queue object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "\n",
    "def tarea(q):\n",
    "    q.put(42)  # Send value to the queue\n",
    "\n",
    "q = Queue()\n",
    "p = Process(target=tarea, args=(q,))\n",
    "p.start()\n",
    "p.join()\n",
    "\n",
    "resultado = q.get() \n",
    "print(f'Received value = {resultado}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE**: When you create multiple processes using multiprocessing. Process, each process runs independently and has its own memory space. This means they cannot easily share results with each other or with the main process. Using processes to parallelize tasks is recommended when the tasks are independent and do not require sharing any information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QuE3eS4_00j"
   },
   "source": [
    "## The Pool Class\n",
    "\n",
    "**The `Pool` class is used to represent a pool of worker processes**. It has methods which can allow you to offload tasks to the worker processes.\n",
    "\n",
    "**It is easier to work with, and higher level, than the `Process` class.**\n",
    "\n",
    "The Master process submit tasks to the workers, the workers perform the tasks, and finally the master retrieves results from the workers.\n",
    "\n",
    "The most used methods in the ``Pool`` Class are (although there are much more):\n",
    "\n",
    "1. Synchronous (blocking) execution: the processes are completed in the same order in which they were started. This is achieved by locking the main program until the respective processes are finished.\n",
    "  - ``Pool.map()`` and ``Pool.starmap()``\n",
    "  - ``Pool.apply()``\n",
    "\n",
    "2. Asynchronous (non-blocking) execution: doesnâ€™t involve locking. As a result, the order of results can get mixed up but usually gets work done quicker.\n",
    "  - ``Pool.map_async()`` and ``Pool.starmap_async()``\n",
    "  - ``Pool.apply_async()``\n",
    "\n",
    "The `map` method is only applicable to a function that accepts a single argument. For routines that accept multiple arguments, the `starmap` method must be used instead. Both versions take an iterable and chunk it into tasks, where every task has the same (mapped) target function.\n",
    "\n",
    "With regards to `apply` and `apply_async`, both take an `args` argument that accepts the parameters passed to the â€˜function-to-be-parallelizedâ€™ as an argument, unlike `map` and similar to `starmap`. However, in this case, `apply` just makes a single call to the function to be parallelized. What does this mean? To really parallelize the funcion **you have to manually iterate the call to `apply` to make use of the pool of workers**. This has the advantage that in each call **you can specify not just a new chunk of data, but also a different task (function) to be executed in that worker**. That is, you can pass a task list and a data list with the arguments for each task.\n",
    "\n",
    "For more info on `apply_async` see:\n",
    "\n",
    "https://stackoverflow.com/questions/53035293/purpose-of-multiprocessing-pool-apply-and-multiprocessing-pool-apply-async\n",
    "\n",
    "https://stackoverflow.com/questions/52985131/how-to-write-a-multithreaded-function-for-processing-different-tasks-concurrentl/52992065#52992065\n",
    "\n",
    "https://docs.python.org/3.8/library/multiprocessing.html#multiprocessing.pool.Pool.apply_async\n",
    "\n",
    "In compute-intensive tasks, typically the problem is to apply the same function to a huge bunch of data, hence, we will give more attention here to `map` and derived methods.\n",
    "\n",
    "Other methods in the multiprocessing library permit the creation of pipes, queues and other approaches, but **the pool of (parallel) workers is by far the most typical and straitghforward approach to parallelize between the cores of a single computer**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSi-dLyq2ArP"
   },
   "outputs": [],
   "source": [
    "# Summary table of Pool class methods\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "#                           |           SINGLE FUNCTION               |  MULTIPLE FUNCTIONS    |\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "#                           |  Single argument |  Multiple arguments  |  Multiple arguments    |\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "# sync process (blocking)   | Pool.map         | Pool.starmap         |  Pool.apply            |\n",
    "#\n",
    "# async proc (non-blocking) | Pool.map_async   | Pool.starmap_async   |  Pool.apply_async      |\n",
    "# ----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JckTK9E8XyYp"
   },
   "source": [
    "### Using Pool.map\n",
    "\n",
    "All multiprocessing `map` functions for a pool of workers behave similar to the standard python `map` function: they execute a specified function for each item in an iterable that it takes as input (both function and iterable):\n",
    "\n",
    "```\n",
    "def square(n):\n",
    "    return n * n\n",
    "\n",
    "num_list = [1,2,3,4]\n",
    "result = map(square, num_list)\n",
    "print('Mapped result is: ', list(result))\n",
    "\n",
    "Output:\n",
    ">> Mapped result is:  [1, 4, 9, 16]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1735820441340,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "2XMtc1iK3BA9",
    "outputId": "8a11ff60-f606-4480-b0dd-e1b02d1732f4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "def doubler(number):\n",
    "  \"\"\"\n",
    "  A doubling function that can be used by a process\n",
    "  \"\"\"\n",
    "  result = number * 2\n",
    "  proc = os.getpid()\n",
    "  print(f'{number} doubled to {result} by process id: {proc}')\n",
    "  return result\n",
    "\n",
    "numbers = [5, 10, 15, 20, 25]\n",
    "\n",
    "# instantiate a pool of 3 processes\n",
    "pool = mp.Pool(processes=3)\n",
    "result = pool.map(doubler, numbers)\n",
    "pool.close()\n",
    "\n",
    "print(f'input data: {numbers}')\n",
    "print(f'results are in corresponding order: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the `pool.map` function works as follows: \n",
    "\n",
    "1. **Step 1: Creating a Pool with 3 processes**\n",
    "2. **Step 2: Assigning Tasks**\n",
    "    - The Pool splits the list [5, 10, 15, 20, 25] into tasks and assigns them to the 3 processes.\n",
    "    - Initially, the 3 processes start working on the first 3 numbers:\n",
    "        * *Process 1*: 5\n",
    "        * *Process 2*: 10\n",
    "        * *Process 3*: 15\n",
    "3. **Step 3: Parallel Execution**\n",
    "    - The 3 processes execute the doubler function in parallel.\n",
    "    - When a process finishes, the Pool assigns it the next available number:\n",
    "        * If *Process 3* finishes first, it is assigned the number 20.\n",
    "        * If *Process 1* finishes next, it is assigned the number 25.\n",
    "\n",
    "4. **Step 4: Collecting Results**\n",
    "    - Even though the processes may finish in different orders, `pool.map` ensures that the results are returned in the correct order:\n",
    "        * The first result corresponds to 5.\n",
    "        * The second result corresponds to 10.\n",
    "        * The third result corresponds to 15.\n",
    "        * The fourth result corresponds to 20.\n",
    "        * The fifth result corresponds to 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qVyfsurW3XC"
   },
   "source": [
    "### Using Pool.map_async\n",
    "\n",
    "`pool.map_async` is the asynchronous version of `pool.map`. This means:\n",
    "\n",
    "- **It does not block the main program**:\n",
    "    * Unlike `pool.map`, which waits for all processes to finish before continuing, `pool.map_async` immediately returns an `AsyncResult` object and allows the main program to continue running.\n",
    "- **Retrieving results**:\n",
    "    * To get the results, you must call the `.get()` method on the `AsyncResult` object. This method blocks the main program until all processes have finished and the results are available.\n",
    "- **Order of results**:\n",
    "    * Although `pool.map_async` is asynchronous, it guarantees that the results are returned in the same order as the input list. This is similar to `pool.map`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1735820529784,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "qkgb8KteTrZg",
    "outputId": "ea6fc203-d39e-464d-8232-68201d18b4ca"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "def doubler(number):\n",
    "  \"\"\"\n",
    "  A doubling function that can be used by a process\n",
    "  \"\"\"\n",
    "  result = number * 2\n",
    "  proc = os.getpid()\n",
    "  print(f'{number} doubled to {result} by process id: {proc}')\n",
    "  return result\n",
    "\n",
    "numbers = [5, 10, 15, 20, 25]\n",
    "\n",
    "# instantiate a pool of 3 processes\n",
    "pool = mp.Pool(processes=3)\n",
    "result = pool.map_async(doubler, numbers)\n",
    "pool.close()  # Don't accept more tasks\n",
    "\n",
    "####### Here we could do some stuff while the processes run in parallel...\n",
    "\n",
    "pool.join()   # Wait for all processes ending\n",
    "results = result.get()# recover the real output data from the result object\n",
    "\n",
    "print(f'input data: {numbers}')\n",
    "print(f'results are in corresponding order: {results}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfT69a1d7PGK"
   },
   "source": [
    "### Using Pool.starmap\n",
    "\n",
    "With `Pool.starmap` instead of a single parameter, multiple parameters are passed as tuples to the function that is being ran in parallel.\n",
    "\n",
    "Hence passing an iterable like `[(1,2), (3,4), ...]` results in `[func(1,2), func(3,4), ...]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1735820973045,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "N_S38um97WeU",
    "outputId": "9ae36cf6-9b37-4819-b519-6cf558d3160b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "def doubler_adder(a, b):\n",
    "  result = a * 2 + b * 2\n",
    "  proc = os.getpid()\n",
    "  print(f'{a}  and {b} doubled and added to {result} by process id: {proc}')\n",
    "  return result\n",
    "\n",
    "# numbers = [5, 10, 15, 20, 25]\n",
    "numbers_in_tuples = [(x,x+1) for x in range(0,10)] # [(0,1), (1,2), ....]\n",
    "\n",
    "# instantiate a pool of 3 processes\n",
    "pool = mp.Pool(processes=3)\n",
    "result = pool.starmap(doubler_adder, numbers_in_tuples)\n",
    "# with a single argument, starmap could also be used:\n",
    "# result = pool.starmap(doubler, [(5,), (10,), (15,), (20,), (25,)])\n",
    "pool.close()\n",
    "\n",
    "print(f'input data: {numbers_in_tuples}')\n",
    "print(f'results are in corresponding order: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekd8lKXX5bAF"
   },
   "source": [
    "Note: You can obtain the same effect using `Pool.map` (instead of `Pool.starmap`), if you take the additional effort of joining together several arguments of the target function in a single argument (data objetct), like in the next example, where 'doubler_adder' function has been modified to take just 1 parameter, which is in fact a list of the two original parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 214,
     "status": "ok",
     "timestamp": 1735821051407,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "8_qh1yE74waW",
    "outputId": "72a1e770-fd24-4805-8d1c-1a75ce1dc1c3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "# function modified to take just a single parameter:\n",
    "def doubler_adder(a):\n",
    "  result = a[0] * 2 + a[1] * 2\n",
    "  proc = os.getpid()\n",
    "  print(f'{a[0]}  and {a[1]} doubled and added to {result} by process id: {proc}')\n",
    "  return result\n",
    "\n",
    "# numbers = [5, 10, 15, 20, 25]\n",
    "numbers_in_list = [[x,x+1] for x in range(0,10)] # [[0,1], [1,2], ....]\n",
    "# instantiate a pool of 3 processes\n",
    "pool = mp.Pool(processes=3)\n",
    "result = pool.map(doubler_adder, numbers_in_list)\n",
    "pool.close()\n",
    "\n",
    "print(f'input data: {numbers_in_list}')\n",
    "print(f'results are in corresponding order: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhHskhSb_vWB"
   },
   "source": [
    "## Comparing execution times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kn4bkAq303WL"
   },
   "source": [
    "The following scripts compare the **blocking multiprocess** approach with the **non-blocking multiprocess** and the **single-process** solution, measuring execution time. Employ a virtual machine with 2 logical cores. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRdx9dZ_jBEs"
   },
   "source": [
    "### 1. Naive approach:\n",
    "\n",
    "No performance gain from parallel execution for I/O bounded tasks, simple tasks, or few tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 711,
     "status": "ok",
     "timestamp": 1735899863244,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "y0Uz-F0LR-lX",
    "outputId": "70e3a907-8882-405e-acf8-8cb6f945cc30"
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def f(x, y):\n",
    "    return (x+y)**(2)\n",
    "\n",
    "# generate 2 arrays of 1 million random integers between 1 and 10\n",
    "x = np.random.randint(1,10,1000000)\n",
    "y = np.random.randint(1,10,1000000)\n",
    "print(f'first values in x: {x[0:10]}')\n",
    "print(f'first values in y: {y[0:10]}')\n",
    "\n",
    "xy_tuple = [(int(x[i]),int(y[i])) for i in range(0,len(x))]\n",
    "print(f'first values in xy_tuple: {xy_tuple[0:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11178,
     "status": "ok",
     "timestamp": 1735821468960,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "XRSIfQixaBiU",
    "outputId": "0e722cb9-6078-42f8-f96f-342317dfd92c"
   },
   "outputs": [],
   "source": [
    "# generate a 2-process pool.starmap\n",
    "pool = mp.Pool(2)\n",
    "# Blocking multiprocess execution\n",
    "t0 = time.time()\n",
    "result1 = pool.starmap(f, xy_tuple)\n",
    "t1 = time.time()\n",
    "pool.close()\n",
    "# print results\n",
    "print(f'first values in result1: {result1[0:10]}')\n",
    "print(f'time for pool.starmap: {t1-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12963,
     "status": "ok",
     "timestamp": 1735821500234,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "Jov29cS_RwEN",
    "outputId": "b79f2070-e259-4324-a76f-00522ea6fdf7"
   },
   "outputs": [],
   "source": [
    "# generate a 2-process pool.starmap_async\n",
    "pool = mp.Pool(2)\n",
    "# Non-blocking multiprocess execution \"in the background\"\n",
    "t0 = time.time()\n",
    "result2_ = pool.starmap_async(f, xy_tuple)\n",
    "pool.close()  # Don't accept more tasks\n",
    "\n",
    "####### Here we could do some stuff while the processes run in parallel...\n",
    "\n",
    "pool.join()   # Wait for all the process ending\n",
    "result2 = result2_.get()# recover the real output data from the result object\n",
    "t1 = time.time()\n",
    "# print results\n",
    "print(f'first values in result2: {result2[0:10]}')\n",
    "print(f'time for pool.starmap_async: {t1-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 833,
     "status": "ok",
     "timestamp": 1735821576008,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "4Dj_tguzBtwb",
    "outputId": "21b98c17-f69f-495d-c678-9cce726bfe1b"
   },
   "outputs": [],
   "source": [
    "# Compare with the single-process solution: send data sequentially\n",
    "result3 = np.zeros(len(x), dtype=int)\n",
    "#result3 = np.zeros(len(x))\n",
    "t0 = time.time()\n",
    "for i in range(len(x)):\n",
    "  result3[i] = f(x[i],y[i])\n",
    "t1 = time.time()\n",
    "print(f'first values in result3: {result3[0:10]}')\n",
    "print(f'time for single-process: {t1-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 187,
     "status": "ok",
     "timestamp": 1735821508288,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "MRwD44slSSEp",
    "outputId": "9f3ffc18-f322-4b0b-cf63-a20b9567d34d"
   },
   "outputs": [],
   "source": [
    "# Compare with the single-process solution: using vectorized operators\n",
    "t0 = time.time()\n",
    "result3 = f(x,y)\n",
    "t1 = time.time()\n",
    "print(f'first values in result3: {result3[0:10]}')\n",
    "print(f'time for single-process: {t1-t0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GKdUhtKP1En"
   },
   "source": [
    "Compare and analyze the times you obtained when executing the different versions of the programs. \n",
    "\n",
    "- At first sight, one would expect the execution time of the parallel versions to be half of that required by the single-process sequential version of the program. Have you observed such a reduction in the time measurements? If not, what is the reason?\n",
    "\n",
    "- By comparing the execution times of the synchronous and asynchronous parallel versions, what conclusions can you draw?\n",
    "\n",
    "- Which version of the program is faster? What are the reasons that make this version the most optimized?\n",
    "\n",
    "\n",
    "> **IMPORTANT NOTE**\n",
    ">- A parallel version of a program can be slower than a sequential one. This can happen when there exists a huge overhead caused by sending data to each process.\n",
    ">- When you define relatively simple tasks (x+y)<sup>2</sup>, the time spent sending data and gathering results is much greater than the time spent on the actual computation.\n",
    ">- To take advantage of parallel processing, we should keep process pool occupied in relatively complex tasks and minimize communication overhead. And, of course, we should use a larger number of physical processors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0gpoNvtjXfO"
   },
   "source": [
    "### 2. Data-chunked version.\n",
    "\n",
    "As a demonstration, check the next code, slighty modified from the previous example:\n",
    "\n",
    "**NOTE**:\n",
    "\n",
    "We will also make use of the \"`with`\" blocks,  referred to as **context managers**. A context manager is a construct that allows you to allocate and release resources automatically when entering and exiting a block of code. The `with` statement ensures that setup and cleanup operations are handled correctly, even if exceptions occur within the block. In this case, it eliminates the need to explicitly call `pool.close()` and `pool.join()` at the end of the parallel section.\n",
    "\n",
    "The use of context managers is generally recomended in python programming, but it is particularly important in parallel programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31442,
     "status": "ok",
     "timestamp": 1736256423560,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "Yn-fUEdG4kGK",
    "outputId": "97d3cea4-f362-4b36-92e5-4b4544dc0ab9"
   },
   "outputs": [],
   "source": [
    "# This example defines a compute-intensive function and sends data in chunks\n",
    "# NOTE: Recommended to test this example in a (virtual) machine with 4 logical cores.\n",
    "import time\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Generate 40 million random floats\n",
    "N = 40_000_000\n",
    "x = np.random.rand(N).astype(np.float64)\n",
    "\n",
    "# Create 4 chunks of 10 million elements each:\n",
    "chunk_size = 10_000_000\n",
    "chunks = [x[i:i + chunk_size] for i in range(0, N, chunk_size)]\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Size of each chunk: {chunk_size}\")\n",
    "\n",
    "# Define a CPU-intensive function\n",
    "def heavy_function(array):\n",
    "    for _ in range(3):\n",
    "        array = (np.sin(array)+np.cos(array))**(array*array)\n",
    "    return np.sum(array)  # Just return the sum of the processed chunk\n",
    "\n",
    "# Multiprocessing - blocking version (Pool.map)\n",
    "def parallel_map(chunks_list, num_procs=4):\n",
    "    with mp.Pool(processes=num_procs) as pool:\n",
    "        results = pool.map(heavy_function, chunks_list)\n",
    "    return results\n",
    "\n",
    "# Multiprocessing - async version (Pool.map_async)\n",
    "def parallel_map_async(chunks_list, num_procs=4):\n",
    "    with mp.Pool(processes=num_procs) as pool:\n",
    "        async_result = pool.map_async(heavy_function, chunks_list)\n",
    "        results = async_result.get()  # Wait for processes to finish\n",
    "    return results\n",
    "\n",
    "# Single-process (serial) execution\n",
    "def serial_execution(chunks_list):\n",
    "    results = []\n",
    "    for chunk in chunks_list:\n",
    "        results.append(heavy_function(chunk))\n",
    "    return results\n",
    "\n",
    "# time all three approaches:\n",
    "# execute and time Parallel blocking code (Pool.map)\n",
    "start_time = time.time()\n",
    "results_map = parallel_map(chunks)\n",
    "end_time = time.time()\n",
    "print(f\"[Pool.map]     Elapsed time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# execute and time Parallel async code (Pool.map_async)\n",
    "start_time = time.time()\n",
    "results_map_async = parallel_map_async(chunks)\n",
    "end_time = time.time()\n",
    "print(f\"[map_async]    Elapsed time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "#execute and time Serial code\n",
    "start_time = time.time()\n",
    "results_serial = serial_execution(chunks)\n",
    "end_time = time.time()\n",
    "print(f\"[Single-process] Elapsed time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_n6x6OYEZT0w"
   },
   "source": [
    "### 3. Another example: prime decompositon\n",
    "\n",
    "A new example, in this case using a more complex task: prime factor decomposition of large numbers.\n",
    "\n",
    "In this case, we are using also `tqdm` to provide a progress bar which supports both single-process and multi-process execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61800,
     "status": "ok",
     "timestamp": 1736256754014,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "Vcc_w-uhMANb",
    "outputId": "0eb348e8-0bf1-4c45-e256-a937c852dae1"
   },
   "outputs": [],
   "source": [
    "# This example uses a function that computes prime factors of a number\n",
    "# NOTE: On a machine with just 1 physical core there won't be performance\n",
    "# gains, use an engine with more cores to perceive the gain in this example \n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a function to decompose a number into its prime factors\n",
    "def prime_factors(n):\n",
    "    factors = []\n",
    "    divisor = 2\n",
    "    while n > 1:\n",
    "        while n % divisor == 0:\n",
    "            factors.append(divisor)\n",
    "            n //= divisor\n",
    "        divisor += 1\n",
    "        if divisor * divisor > n and n > 1:\n",
    "            factors.append(n)\n",
    "            break\n",
    "    return factors\n",
    "\n",
    "# Create data to process (large numbers for factorization)\n",
    "# create a list of 100 random floating point numbers between 1e14 and 1e18\n",
    "data = np.random.uniform(1e10, 1e12, 100)\n",
    "\n",
    "# Single-process execution\n",
    "def single_process_execution(data):\n",
    "    results = []\n",
    "    for number in tqdm(data, desc=\"Single-process execution\"):\n",
    "        results.append(prime_factors(number))\n",
    "    return results\n",
    "\n",
    "# Multi-process execution with starmap\n",
    "def multi_process_execution(data):\n",
    "    with Pool(2) as pool:  # Use 2 processes\n",
    "        results = list(tqdm(pool.map(prime_factors, data), total=len(data), desc=\"Multi-process execution\"))\n",
    "    return results\n",
    "\n",
    "# Measure time for single process execution\n",
    "start_time = time.time()\n",
    "single_results = single_process_execution(data)\n",
    "single_duration = time.time() - start_time\n",
    "print(f\"Single-process execution time: {single_duration:.2f} seconds\")\n",
    "\n",
    "# Measure time for multi-process execution\n",
    "start_time = time.time()\n",
    "multi_results = multi_process_execution(data)\n",
    "multi_duration = time.time() - start_time\n",
    "print(f\"Multi-process execution time (2 cores): {multi_duration:.2f} seconds\")\n",
    "\n",
    "# Verify that the results are identical\n",
    "assert single_results == multi_results, \"Results do not match!\"\n",
    "print(\"Results are identical.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZD8fm_IVO9B"
   },
   "source": [
    "## Running several python scripts in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 170,
     "status": "ok",
     "timestamp": 1735913679761,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "Io__C0k5IIKL",
    "outputId": "621c86ec-a7c7-4b61-f0bc-24e950f19e5e"
   },
   "outputs": [],
   "source": [
    "%%file script1.py\n",
    "import os\n",
    "print(f'hello from script 1, executed by process {os.getpid()}.')\n",
    "f= open(\"file1.txt\",\"w+\")\n",
    "f.write(\"hello from script 1\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1735913681145,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "o4orpbP-IUjn",
    "outputId": "c425e7c7-ae2c-4ee1-e6e4-ff8cf8152ac4"
   },
   "outputs": [],
   "source": [
    "%%file script2.py\n",
    "import os\n",
    "\n",
    "print(f'hello from script 2, executed by process {os.getpid()}.')\n",
    "f= open(\"file2.txt\",\"w+\")\n",
    "f.write(\"hello from script 2\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1735913682729,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "hBeCEqopIVP2",
    "outputId": "440c3815-2ac3-47f5-c7fa-59e1b5a79010"
   },
   "outputs": [],
   "source": [
    "%%file script3.py\n",
    "import os\n",
    "\n",
    "print(f'hello from script 3, executed by process {os.getpid()}.')\n",
    "f= open(\"file3.txt\",\"w+\")\n",
    "f.write(\"hello from script 3\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJ8PZQ6DfZfa"
   },
   "source": [
    " Now, run 3 processes so that each process executes one of the python scripts in parallel with the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1735913686850,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "zi_xB8mxIZxG",
    "outputId": "579dd442-e24c-454b-8f87-d0f606edbf03"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing as mp\n",
    "import subprocess\n",
    "\n",
    "script_list = ['script1.py', 'script2.py', 'script3.py']\n",
    "\n",
    "def run_python(process):\n",
    "  result = subprocess.run([\"python\", process], capture_output=True, text=True)\n",
    "  return result.stdout\n",
    "\n",
    "pool = mp.Pool(processes=3)\n",
    "results = pool.map(run_python, script_list)\n",
    "pool.close()\n",
    "\n",
    "print(f'results = {results}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-g9dOOv7GSf"
   },
   "source": [
    "## Using Pool.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LPXiHejexwt"
   },
   "source": [
    "The Pool.map and Pool.apply will lock the main program until all processes are finished, which is quite useful if we want to obtain results in a particular order for certain applications.\n",
    "\n",
    "In contrast, the async variants will submit all processes at once and retrieve the results as soon as they are finished. One more difference is that we need to use the get method after the apply_async() call in order to obtain the return values of the finished processes.\n",
    "\n",
    "The order of the results is not guaranteed to be the same as the order of the calls to Pool.apply_async.\n",
    "\n",
    "Notice also that you could call a number of different functions with Pool.apply_async (not all calls need to use the same function). In contrast, Pool.map applies the same function to many arguments.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1734627688773,
     "user": {
      "displayName": "akk Kkk",
      "userId": "11030245060253430183"
     },
     "user_tz": -60
    },
    "id": "PLguzlQZ-rbu",
    "outputId": "326f1bf3-6c15-4280-d98a-9283caef1904"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def doubler(number):\n",
    "  \"\"\"\n",
    "  A doubling function that can be used by a process\n",
    "  \"\"\"\n",
    "  result = number * 2\n",
    "  proc = os.getpid()\n",
    "  print(f'{number} doubled to {result} by process id: {proc}')\n",
    "  return result\n",
    "\n",
    "numbers = [5, 10, 15, 20, 25]\n",
    "\n",
    "results =[]\n",
    "pool = Pool(processes=3)\n",
    "for i,number in enumerate(numbers): # note the for-loop!!! calling apply_async creates just a single process\n",
    "  results.append(pool.apply_async(doubler, (numbers[i],)).get(timeout=1))\n",
    "\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
